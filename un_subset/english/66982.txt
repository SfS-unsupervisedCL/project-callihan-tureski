%%%PROGRAMME EVALUATION|PROGRAMME PLANNING|INTERNAL OVERSIGHT|

Sixty-eighth session * A/68/50. Agenda items 137 and 144 and of the preliminary list* Programme planning Report on the activities of the Office of Internal Oversight Services Strengthening the role of evaluation and the application of evaluation findings on programme design, delivery and policy directives Report of the Office of Internal Oversight Services Summary In accordance with regulation 7.4 of the Regulations and Rules Governing Programme Planning, the Programme Aspects of the Budget, the Monitoring of Implementation and the Methods of Evaluation (ST/SGB/2000/8), the present report is the thirteenth in a series of studies that have been submitted to the General Assembly through the Committee for Programme and Coordination. Using both qualitative and quantitative methods and covering the 2010-2011 biennium, the Office of Internal Oversight Services (OIOS) assessed the Secretariat's evaluation capacity, quality and utility and identified key results contained in evaluation reports. This biennial study also presents the OIOS evaluation workplan for 2014-2015 and describes OIOS follow-up to prior Committee recommendations aimed at improving the quality of Secretariat evaluations. Despite the degree of progress made during the prior biennium, such as greater integration of gender perspectives into evaluation, overall evaluation capacity in the Secretariat remains uneven and inadequate. Evaluation resources are still insufficient, and the organizational framework, culture and commitment needed to promote and facilitate a comprehensive evaluation function that provides critical, timely and strategic information for decision-making and strengthens accountability and learning are lacking. Important processes for evaluation planning, conduct and follow-up are not systematically implemented, evaluation policies are not always in place, and the competencies of staff conducting evaluations are uneven. Furthermore, some focal points reported a lack of the management support for and buy-in of evaluation that are necessary for the building of a strong evaluation culture. These factors have contributed to the limited utility of evaluation, with significant gaps in evaluation coverage causing large areas of the Organization to lack evaluative evidence on performance to guide strategic decision-making. Since the previous biennium, the quality of evaluation reports has improved marginally, with 49 per cent of reports reviewed assessed as being of excellent or good quality overall. At the operational level, there is a need for more self-evaluation support in the form of training, the sharing of lessons learned and technical guidance. In its resolution 67/226, the General Assembly emphasized the importance of having independent, credible and useful evaluation functions, with sufficient resources, and a culture of evaluation that ensures the active use of evaluation findings and recommendations in policy development and improving the functioning of the organizations of the United Nations. This ambition has not yet been reached in the Secretariat, where evaluation has yet to become a fully robust function that is integral to how the Organization operates. To that end, the critical gaps identified above must be addressed. Page Introduction Methodology Study results Evaluation capacity in the Secretariat remains uneven and inadequate Overall evaluation productivity and quality in the Secretariat have remained stable, and there is still significant room for improvement to enhance the quality of the evaluations conducted Evaluation has not yet reached its full potential with regard to utility Evaluations conducted during the previous biennium identified several issues in the eight strategic priority areas of the Organization Evaluation workplan of the Office of Internal Oversight Services Follow-up on the recommended actions of the Committee for Programme and Coordination Conclusions Annexes List of departments and offices included in the biennial study Methodology for the review of evaluation reports Comments received from Secretariat entities on the draft report 1. The present report is the thirteenth in a series of studies that have been submitted biennially since 1988 to the General Assembly through the Committee for Programme and Coordination, in accordance with the Regulations and Rules Governing Programme Planning, the Programme Aspects of the Budget, the Monitoring of Implementation and the Methods of Evaluation (ST/SGB/2000/8). The report was reviewed by Secretariat departments and offices, and their comments were incorporated as appropriate. 2. The purpose of this biennial study was to describe and assess the status of evaluation in the United Nations Secretariat, as well as to identify the key issues emerging from evaluations conducted during the 2010-2011 biennium. It was focused specifically on: (a) The current capacity, quality and utility of the evaluation function within and across the Secretariat; (b) Key results and conclusions contained in evaluation reports finalized during the biennium 2010-2011. 3. In addition, the study presents the evaluation workplan of the Office of Internal Oversight Services (OIOS) and reports on OIOS follow-up to recommendations made by the Committee for Programme and Coordination following the issuance of the previous biennial study. OIOS plans a follow-up report containing programme-by-programme scorecards, including more detailed data regarding evaluation capacity and practice. 4. In concluding the twelfth biennial Office of Internal Oversight Services study, the Committee for Programme and Coordination emphasized that evaluation was a key function for the adoption of budgetary decisions, since it not only helped to improve programme design and execution, as well as the formulation of policy directives, but also contributed to transparency, effective implementation of intergovernmental mandates and the maximization of the use of resources. At the same time, it allowed Member States to follow up on programme outcomes in a systematic way. 5. In the context of that conclusion, and on the basis of the United Nations Evaluation Group (UNEG) Norms and Standards for Evaluation in the United Nations system, evaluation in the Secretariat should serve two fundamental purposes: (a) to provide for programme accountability to Member States, senior leadership and beneficiaries; and (b) to determine programme effectiveness and lessons learned with a view to programme improvement. A comprehensive and robust evaluation function answers the following three critical questions: Are we doing the right things? Are we doing them right? Are we doing them on a sufficient scale to make a difference? 6. There are two types of evaluations in the Secretariat: (a) independent evaluations undertaken by OIOS, in which the evaluation function is external to the programmes being evaluated; and (b) self-evaluation undertaken by the programmes themselves, either as part of a dedicated evaluation unit or otherwise, in which the evaluation function is embedded within the programmes being evaluated. OIOS also undertakes cross-cutting thematic evaluations that address issues of strategic importance across the Organization. Both types of evaluation are complementary and necessary to ensure a comprehensive evaluation approach throughout the Secretariat. II. Methodology 7. This study was conducted using the following six methods: (a) A quantitative and qualitative analysis of evaluation reports finalized in 2010-2011 to determine their attributes, quality and key results and conclusions (see annex II for information on how those reports were collected and selected for the analyses); (b) A web-based survey of Secretariat programmes. The survey was conducted from November to December 2012 and had a 100 per cent response rate; (c) In-person or telephone interviews with focal points; (d) An assessment of evaluation policies; (e) An assessment of evaluation budgets; (f) Workshops with focal points to discuss evaluation capacity benchmarks. 8. One limitation of the study was that OIOS might not have received all evaluation reports finalized during 2010-2011, although the Office guided focal points on the submission of reports from their respective entities. An additional limitation was that the non-random sampling of reports to determine their attributes, quality and content may have limited the extent to which the study results can be generalized, including the comparison of data among the past three bienniums. Finally, the quality of evaluation was determined primarily through an assessment of evaluation reports. 9. To balance those potential limitations, all analyses were triangulated with data from the data sources identified in paragraph 7 above, in order to strengthen the study results. III. Study results Current evaluation capacity, quality and utility A. Evaluation capacity in the Secretariat remains uneven and inadequate 10. Among its conclusions regarding the previous biennial study, the Committee for Programme and Coordination stated: "The Committee stressed that appropriately balanced competencies and strong commitment from staff at all managerial levels, including senior leadership support, as well as sufficient financial and staffing resources, were among the main elements required to ensure adequate conduct of evaluation activities in the Secretariat." 11. In order to assess current evaluation capacity in the Secretariat, OIOS reviewed the following key indicators, each of which is discussed below: :: Resources :: Structure :: Processes :: Culture :: Competencies. 12. Despite the degree of progress made during the prior biennium, overall capacity for evaluation in the Secretariat remains inadequate. Evaluation resources are still insufficient, and the organizational framework and culture needed to promote and facilitate a comprehensive evaluation function that provides critical and timely information to inform decision-making and strengthen accountability and results are lacking. Thus, the current commitment to evaluation in the Secretariat is weak; without such commitment, steps cannot be taken to strengthen the function so that it makes a difference in achieving organizational results. Resources 13. The General Assembly, in its resolution 58/269, requested the Secretary-General to ensure that resources are clearly identified in all the sections of the proposed programme budget for the performance of the monitoring and evaluation functions. Each entity submits budget Form 12, indicating resources needed to conduct mandatory self-assessments and discretionary internal evaluations. 14. Monitoring and evaluation resources continue to be insufficient. The Secretariat entities had earmarked 0.29 per cent of their overall budget for monitoring and evaluation. This is 0.07 per cent less than in the previous biennium. Of the Secretariat entities, while 17 had proposed budgets of more than $1 million for monitoring and evaluation, only 10 (one third) spent more than 1 per cent of their total programme budget on those functions. Planned evaluation and monitoring resources for 2012-2013 amounted to $54.8 million, representing 0.33 per cent of the total budget. This is a small relative increase of 0.04 per cent compared with 2010-2011, but lower than the 0.36 per cent of the total budget reflected in 2008-2009. 15. As illustrated in figure 1, when the percentage of the budget allocated to monitoring and evaluation in 2010-2011 is compared with that in 2006-2007, 10 entities had lower shares in 2010-2011. Fourteen spent a lower share of their budget on monitoring and evaluation in 2010-2011 than in 2008-2009. Figure I Percentage of budgets of Secretariat entities devoted to monitoring and evaluation, 2006-2011 Source: OIOS analysis using data contained in budget fascicles for 2010-2011 (A/64/6) and figures provided by the Office of Programme Planning, Budget and Accounts. Abbreviations: DESA, Department of Economic and Social Affairs; DGACM, Department for General Assembly and Conference Management; DM, Department of Management; DPA, Department of Political Affairs; DPI, Department of Public Information; DPKO, Department of Peacekeeping Operations; DSS, Department of Safety and Security; ECA, Economic Commission for Africa; ECE, Economic Commission for Europe; ECLAC, Economic Commission for Latin America and the Caribbean; ESCAP, Economic and Social Commission for Asia and the Pacific; ITC, International Trade Centre; OCHA, Office for the Coordination of Humanitarian Affairs; OCSS, Office of Central Support Services; ODA, Office for Disarmament Affairs; OHCHR, Office of the United Nations High Commissioner for Human Rights; OHRLLS, Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States; OHRM, Office of Human Resources Management; OICT, Office of Information and Communications Technology; OLA, Office of Legal Affairs; OOSA, Office for Outer Space Affairs; OPPBA, Office of Programme Planning, Budget and Accounts; OSAA, Office of the Special Adviser on Africa; OUSG, Office of the Under-Secretary-General; UNCTAD, United Nations Conference on Trade and Development; UNEP, United Nations Environment Programme; UN-Habitat, United Nations Human Settlements Programme; UNHCR, Office of the United Nations High Commissioner for Refugees; UNODC, United Nations Office on Drugs and Crime; UNRWA, United Nations Relief and Works Agency for Palestine Refugees in the Near East. 16. Given the lack of a clear and common understanding of self-evaluation on the part of Secretariat entities, with few exceptions, it continues to be challenging to isolate the resources dedicated solely to evaluation. On the basis of the proposed estimation of work-months submitted on Form 12 for the biennium 2010-2011, it was reported that 53 per cent of overall evaluation resources were dedicated to mandatory self-assessment activities, with 47 per cent dedicated to discretionary self-evaluation activities. This implies that the ratio of evaluation resources to total budget is approximately 0.14 per cent. Of the programmes analysed, only three spent more than 1 per cent of their budget on evaluation and only about one fourth (eight) had evaluation budgets of more than $1 million. However, judging from focal point interviews, no standard method has been established for differentiating between monitoring and evaluation, and thus there is great variation in how the proposed budgets for monitoring and evaluation are reported on Form 12. Many proposed "discretionary self-evaluation" activities are management or operational activities. One programme indicated a budget of almost $3 million for monitoring and evaluation, but its Form 12 showed a significant number of work-months (24 months at the D level, 12 months at the P-5 level and 57 months at the P-4 and P-3 levels) for activities ranging from review meetings and lesson-sharing to the review of reports provided to intergovernmental bodies and staff training. Thus, the actual proportion of resources devoted to self-evaluation in the Secretariat is far lower than the estimated 0.14 per cent of the total budget. Structure 17. As illustrated in the table below, 18 Secretariat programmes had a stand-alone unit dedicated to evaluation, either exclusively or in conjunction with another management function, such as monitoring, programme planning or policy development. Of the 18 programmes with a stand-alone unit, however, only 6 were devoted exclusively to evaluation. According to focal points, the number of unit staff varied from 1 to 10, with none higher than the P-5 level. Only 6 of the 18 units reported directly to the department head. Furthermore, focal points in 18 programmes reported having programme staff who, while not working in a dedicated evaluation unit, were engaged in evaluation on a part-time basis. 18. The lack of a dedicated unit for evaluation in programmes carrying out significant operational activities and contrasted with significant mandates, such as the Department of Political Affairs and the Department of Economic and Social Affairs, is of serious concern. In the forthcoming evaluation scorecards, OIOS will provide further information and suggestions regarding how to fill this gap. Unit dedicated to evaluation only Unit dedicated to evaluation and other functions No unit dedicated to evaluation DPI DGACM DESA DPKO/DFSa ECA DM UNEP ECE DPA UNODC ECLAC DSS UNRWA ESCAP EOSG UN-Women ESCWA ODA ITC OHRLLS OHCHR OLA UNCTAD OOSA UNHCR OSAA UN-Habitat UNOG OCHA UNON UNOV Source: OIOS biennial focal point surveys. OIOS was not included in this analysis. Note: Highlighted evaluation units report directly to the programme head. Abbreviations: DESA, Department of Economic and Social Affairs; DFS, Department of Field Support; DGACM, Department for General Assembly and Conference Management; DM, Department of Management; DPA, Department of Political Affairs; DPI, Department of Public Information; DPKO, Department of Peacekeeping Operations; DSS, Department of Safety and Security; ECA, Economic Commission for Africa; ECE, Economic Commission for Europe; ECLAC, Economic Commission for Latin America and the Caribbean; EOSG, Executive Office of the Secretary-General; ESCAP, Economic and Social Commission for Asia and the Pacific; ITC, International Trade Centre; OCHA, Office for the Coordination of Humanitarian Affairs; ODA, Office for Disarmament Affairs; OHCHR, Office of the United Nations High Commissioner for Human Rights; OHRLLS, Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States; OLA, Office of Legal Affairs; OOSA, Office for Outer Space Affairs; OSAA, Office of the Special Adviser on Africa; UNCTAD, United Nations Conference on Trade and Development; UNEP, United Nations Environment Programme; UN-Habitat, United Nations Human Settlements Programme; UNHCR, Office of the United Nations High Commissioner for Refugees; UNODC, United Nations Office on Drugs and Crime; UNOG, United Nations Office at Geneva; UNON, United Nations Office at Nairobi; UNOV, United Nations Office at Vienna; UNRWA, United Nations Relief and Works Agency for Palestine Refugees in the Near East; UN-Women, United Nations Entity for Gender Equality and the Empowerment of Women. a DPKO and DFS have a shared evaluation unit. Competencies 19. The professional backgrounds of staff responsible for evaluation varied greatly. Only one third of focal points interviewed indicated that these staff had evaluation backgrounds that had equipped them with specific professional competencies and skills. For these programmes, the average number of years of staff evaluation experience was eight. For the remaining programmes, staff responsible for evaluation typically had backgrounds in programme management, finance, economics and other social sciences. Processes 20. While the Regulations and Rules Governing Programme Planning, the Programme Aspects of the Budget, the Monitoring of Implementation and the Methods of Evaluation establish a broad framework for evaluation in the Secretariat, an entity-specific evaluation policy establishes a clear and strong identity for the function. As illustrated in figure II, the number of entities with an evaluation policy has increased over the past three bienniums, although not substantially over the past two: from 9 in 2006-2007 to 15 in 2008-2009 and 17 in 2010-2011. Three more programmes (the Department of Economic and Social Affairs, the Department of Political Affairs and ECA) developed evaluation policies in 2012, and the United Nations Entity for Gender Equality and the Empowerment of Women (UN-Women) developed its policy in 2013. It is noteworthy that 11 programmes still lack an evaluation policy. Figure II Number of entities with evaluation policies, 2006-2011 Source: OIOS review of evaluation policies. a The number of entities reviewed for the bienniums 2006-2007 and 2008-2009 was 31. 21. The evaluation policies generally covered the range of UNEG standards established for such policies. The average rating for programmes was 1.5 on a three-point scale, in which 0 means "no UNEG standards met", 1 means "some UNEG standards met" and 2 means "all UNEG standards met". All 17 policies explained the concept and the role of evaluation. Furthermore, most policies (85 per cent or more) identified a mechanism for follow-up to evaluation results, described how evaluations feed into organizational learning strategies and addressed evaluation planning, although only approximately half stated the criteria utilized in selecting evaluation topics. The greatest weakness was that more than one third (40 per cent) did not clearly identify evaluation staff competencies. 22. With regard to establishing an evaluation agenda that meets organizational needs for strategic and timely information, a majority of focal point survey respondents (68 per cent) reported having an evaluative framework for their programmes, approved by the General Assembly in the context of the strategic framework, that identified related inputs, activities, outputs, outcomes and impacts. However, fewer than half of those respondents (48 per cent) had a formal evaluation plan for the biennium 2010-2011, although of those who did, most reported having implemented 75 per cent or more of the plan. 23. As reported in the context of focal point surveys, critical processes for planning, conducting and following up on evaluation are still lacking in a large number of programmes. The implementation of these processes has remained relatively constant over the past two bienniums, as illustrated in figure III. 24. With regard to critical evaluation processes, the results of the evaluation policy analysis reported in paragraph 20 above were more positive than the results of the focal point survey reported in paragraph 22. There are two reasons for this: (a) the focal point survey covered 32 Secretariat entities, compared with the 17 covered by the policy review; and (b) the fact that evaluation processes are referred to in policies does not necessarily indicate that those processes are implemented. Figure III Evaluation procedures as reported by Secretariat focal points, 2008-2011 Source: OIOS biennial focal point surveys. 25. Several trends have emerged over the past two bienniums. On the positive side, more focal points report the integration of gender into the design and process of evaluation (45 per cent in 2010-2011, compared with 32 per cent in 2008-2009), and more programmes also report the integration of human rights into their evaluations, although the level of such integration remains inadequate (26 per cent in 2010-2011 compared with 13 per cent in 2008-2009). On the negative side, fewer focal point survey respondents reported the formal sharing and/or dissemination of evaluation reports (45 per cent in 2010-2011, a significant decrease compared with 71 per cent in 2008-2009). Focal points interviewed identified various ways in which evaluation results were shared, including formal and informal meetings with programme managers, website publication and management responses. They suggested that evaluation results could be better disseminated through formal communication strategies and better knowledge management. In fact, fewer than half of focal point survey respondents (42 per cent) reported having a formal procedure for the dissemination of lessons learned from evaluations. Culture 26. The Secretariat evaluation culture remains insufficient to foster a robust function in which evaluation is seen as adding value to organizational performance; this insufficiency contributes to the low amount of resources devoted to evaluation, the limited degree of dedicated and independent evaluation capacity and the absence of an evaluation policy in half of the entities of the Secretariat. Focal point survey respondents observed that lack of buy-in on the part of management was one of the biggest obstacles that their programmes faced in conducting and utilizing evaluation in 2010-2011, and the need to strengthen management support was cited as a critical factor that needed to be addressed in order to strengthen evaluation. This has not changed since the biennial study for 2008-2009, in which it was reported that evaluation had yet to become a fully accepted management function. 27. Interviews with focal points revealed that lack of an evaluation culture was an impediment to strong evaluation. One third of respondents made this point, stating that the lack of an evaluation culture was having adverse effects on efforts to move towards a results-based environment. They also stated that a lack of human and financial resources prevented the building of a culture that supported and promoted evaluation. In addition, some suggested that a lack of attention on the part of senior management with respect to evaluation resulted in sporadic and non-systematic evaluation. Their recommendations for strengthening the evaluation culture in the Organization included the setting and enforcement of standards across the Secretariat for evaluation planning, resource requirements and the dissemination of evaluation results. 28. Focal points identified the need for greater self-evaluation support, such as more training, specifically on the practical application of evaluation; more sharing of lessons learned to feed into programme design and implementation; and greater technical guidance on the conduct of evaluation. 29. Focal points also suggested that there was a need for greater liaison with OIOS. This included the Office advocating and promoting evaluation in the Secretariat as well as providing guidance on Secretariat-wide standards, derived from UNEG norms and standards, for the conduct of evaluation. B. Overall evaluation productivity and quality in the Secretariat have remained stable, and there is still significant room for improvement to enhance the quality of the evaluations conducted 30. Overall, the number of evaluation reports produced within the Secretariat has been stable. In 2010-2011, OIOS determined that there were 153 evaluations across the Secretariat, compared with 155 in 2008-2009 and 168 in 2006-2007. However, evaluation activity is more concentrated in a few entities. In 2010-2011, the top five entities in terms of evaluation output accounted for 77 per cent of all evaluation reports; in comparison, in 2008-2009 the top five accounted for 46 per cent of all reports, and in 2006-2007 they accounted for 65 per cent. Thus, while overall numbers have remained constant, there are significant gaps in evaluation coverage, with large parts of the Organization not subject to evaluation and therefore lacking evaluative evidence on performance to guide strategic decision-making. 31. As shown in figure IV, there has been only marginal improvement in the quality of these reports. The overall report quality rating was 2.48 in 2010-2011, compared with 2.62 in 2008-2009, based on a five-point scale in which 1 means "excellent" and 5 means "very poor". There was some general correlation between evaluation resources and evaluation quality. For example, the programme with the largest amount of evaluation resources received the highest overall quality score. Figure IV Overall evaluation quality, 2008-2011 Source: OIOS assessment of the quality of Secretariat evaluation reports. No evaluation reports were rated "very poor" overall. 32. With respect to the various sections of evaluation reports, some were rated higher than others, as shown in table 3. The overall assessment of the "Background" section of evaluation reports was the highest, with 68 per cent rated "excellent" or "good", while the "Conclusions" section received the lowest overall rating, with only 38 per cent rated "excellent" or "good" across the Secretariat. Source: OIOS assessment of the quality of Secretariat evaluation reports. 33. In their "Introduction" section, more than over 80 per cent of the reports directly introduced the subject and 60 per cent clearly stated the purpose and objective of the evaluation. Fewer reports (approximately half) had a clearly and adequately defined scope and set out clear criteria and questions to be addressed in the report. 34. With regard to evaluation methodologies, the majority of the reports (60 per cent) presented them in a convincing manner, adequately describing data sources and analyses. As for the rigour of the evaluation design, just over one half (52 per cent) of reports were rated highly for containing appropriate and adequate evaluation criteria and questions. In contrast, descriptions of methodological limitations and evaluation validity received the lowest overall score, with a quarter of the reports rated as "poor" or "very poor" in terms of those two criteria. 35. With regard to evaluation results, most reports (75 per cent) presented them clearly and objectively, with a majority (56 per cent) presenting sufficient evidence to support them. In those reports rated more poorly on evaluation results, the main weaknesses identified were that they were presented in too narrative a style, were supported with weak evidence and were not adequately synthesized. In these reports, key findings were not clearly stated, enabling and constraining factors were not given sufficient attention, and opinions were inappropriately included among findings. 36. With regard to the "Conclusions" section of evaluation reports, the majority of the reports contained conclusions based on findings, but less than 40 per cent contained conclusions that addressed larger questions and issues or added value to the reports; these conclusions primarily reiterated the evaluation results, at times in an almost random way. The lack of conclusions that contributed to the reports by presenting analysis and interpretation of the results was the largest overall weakness identified among Secretariat evaluation reports in the biennium 2010-2011. 37. In their "Recommendations" section, three quarters of the reports set out recommendations that were directly related to results and conclusions. However, more than one third of the recommendations contained in the reports did not specify who was responsible for implementing them. 38. The overall quality ratings did not vary significantly between independent OIOS evaluations and programme self-evaluations. OIOS reports scored higher than reports of other programmes on background, conclusions, recommendations and format. OIOS conclusions built upon and added value to the findings and presented an evaluator's view more than did the conclusions set out in reports of other programmes. OIOS evaluation recommendations were also better in terms of specifying who would be responsible for implementation. However, OIOS reports scored lower on the "Introduction" section, particularly with regard to setting out criteria and questions. C. Evaluation has not yet reached its full potential with regard to utility 39. A utilization-focused approach to evaluation is one that fosters evidence-based decision-making and strategic planning, enhances the usefulness of evaluation results and recommendations and generates relevant and timely products. Evaluation must be utility-focused in order to facilitate stakeholder ownership. By focusing on the information needs of intended users, evaluations can enhance the building of robust evidence to support a knowledge platform for policy advice and the replication of innovative initiatives. Most evaluations conducted in 2010-2011 were at the project level, but placed a greater focus on outcomes 40. Figures VI and VII compare evaluations conducted within the Secretariat over the past three bienniums in terms of scope and focus. The overall scope of evaluation in the Secretariat remains primarily at the discrete project level, and cross-cutting evaluations are still relatively infrequent. It is encouraging that there continues to be a positive trend towards an increased focus on programme implementation and outcomes, with more than 90 per cent of the evaluations conducted during the past two bienniums focusing on those criteria, a significant increase compared with the biennium 2006-2007. This trend speaks positively about the potential for greater evaluation utility. Figure VII Focus of evaluations as reported by Secretariat focal points, 2006-2011 Evaluations placed little focus on efficiency, and gender and human rights were not well integrated 41. The review of evaluation reports issued during the biennium noted several weaknesses with regard to evaluation coverage. One weakness concerned the evaluation of programme efficiency: very few reports did this in a rigorous manner, and this represented a methodological challenge across the Secretariat. Furthermore, with regard to the mainstreaming of human rights and gender perspectives into evaluation, the former is still lacking, with almost no integration of human rights perspectives into the evaluation methodologies and findings presented in the reviewed evaluation reports from the biennium 2010-2011. Greater progress has been made on gender mainstreaming, as reflected in the evaluation results contained in nearly half of the reports reviewed. Use of evaluation, including systematic follow-up, remains limited 42. While there has been some increase in the reported use of evaluation for programme improvement and senior management reporting, the utilization of evaluation information in the Secretariat has not significantly increased over the past two bienniums. As reported by focal point survey respondents and illustrated in figure VIII, evaluation is being used for a variety of purposes; however, at least one third of programmes did not use evaluation for any of those purposes. 43. Interviews and workshops with focal points explored the reasons behind the limited use of evaluation. As previously noted with regard to the lack of a robust evaluation culture, those reasons included poor management buy-in, limited resources, a lack of dedicated evaluation capacity and an attitude among managers that evaluation is a burden rather than a value-added activity. Focal points referred to the low visibility and consistent application of UNEG norms and standards as further challenges. Source: OIOS biennial focal point surveys. Data for 2006-2007 were not available. 44. Timely follow-up to evaluations through the implementation of recommendations is a critical component of the use of evaluation for accountability, and such follow-up is not being carried out consistently across the Secretariat. As shown in figure III, less than half of focal point survey respondents (48 per cent) reported that their entities had developed an action plan for the implementation of evaluation recommendations, with only 39 per cent indicating that they carried out formal tracking/monitoring of the implementation of recommendations set out in the biennial evaluation plan. 45. The sharing of evaluation results is another critical component of the use of evaluation for learning. As shown in figure III, this also remains a weak area in the utilization of evaluations in the Secretariat. Only 45 per cent of focal point survey respondents reported formal procedures for the sharing and dissemination of evaluation reports, with 42 per cent reporting procedures for the sharing and dissemination of lessons learned from evaluations. 46. Focal point survey respondents cited specific examples of the ways in which evaluation had made a difference in their entities, indicating the potential of evaluation for the strengthening of programme performance. UN-Women and the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA) reported that evaluations had enhanced their strategic framework planning; UN-Women also reported that an evaluability assessment of the strategic plan of the former United Nations Development Fund for Women (UNIFEM) had been used to inform the development of the UN-Women strategic plan for 2011-2013 by indicating main areas for strengthening management, including the improvement of management efficiency. Furthermore, an evaluation of the Economic and Social Commission for Asia and the Pacific (ESCAP) fostered closer working relations between the Commission's headquarters and its subregional offices. The Department of Economic and Social Affairs reported that evaluation had led to improvements in terms of service delivery and customer satisfaction, while in the Department of Public Information, an evaluation reportedly had led to the strengthening of an important website to make it more interactive with target audiences and enable it to better meet the needs of key stakeholders. D. Evaluations conducted during the previous biennium identified several issues in the eight strategic priority areas of the Organization 47. From the qualitative assessments of Secretariat evaluations conducted during the biennium 2010-2011, several themes emerged with regard to organizational performance. These are discussed individually below under the eight strategic priority areas of the Organization. It is worth noting that the majority of reports presented largely positive evaluation results, with only a few presenting negative results. Promotion of sustained economic growth and sustainable development 48. In the area of the promotion of sustained economic growth and sustainable development, 26 reports, out of the total of 60 reviewed, received a "good" or "excellent" quality rating and were analysed in terms of their content. This category comprised the largest number of reports. Of these 26, the majority (20), presented largely positive results. 49. Most of these reports provided detailed results at the project level and were therefore limited with regard to Organization-wide, cross-cutting conclusions about ways in which the Secretariat was promoting sustained economic growth and sustainable development. Key points emerging from these evaluations included the following: (a) The Organization had positively influenced policy discussions, decision-making and policy implementation; (b) Relevance and competitiveness had been lost to younger and better-resourced regional and international organizations, including other United Nations agencies; (c) The Organization provided an effective framework for regional cooperation and could help to shape institutional requirements over the long term; (d) Tools in the area of trade improved decision-making, services and the design of trade policies in developing countries; (e) The Organization was effectively functioning as a development research office, but had had limited success in directly influencing intergovernmental debates; (f) The World Urban Forum had become the world's premier platform for urban development practitioners, policymakers, national and local governments, non-governmental actors, researchers and youth and women's groups to discuss urban issues; (g) In the areas of water and sanitation, the Organization had achieved significant results as a social and technical model tester and service provider at the community and municipal levels. Effective coordination of humanitarian assistance efforts 50. In the area of the effective coordination of humanitarian assistance efforts, 14 reports received a "good" or "excellent" quality rating and were analysed in terms of their content. Of these, the majority (9) presented largely mixed results. Key points emerging from these evaluations included the following: (a) United Nations humanitarian coordination work had resulted in better-defined roles and responsibilities in humanitarian responses and, in some cases, stronger humanitarian partners; (b) In some cases, humanitarian assistance that had been delivered was insufficiently accessible to all affected populations, in particular families headed by women; (c) Effective financial mechanisms were generally available for the timely delivery of assistance, especially in initial emergency phases; (d) Weaknesses of United Nations humanitarian work included poor needs assessments and gaps between relief efforts and recovery work, limiting the long-term impacts of the interventions. Drug control, crime prevention and combating international terrorism 51. In the area of drug control, crime prevention and combating international terrorism, seven reports received a "good" or "excellent" quality rating and were analysed in terms of their content. Of these, the majority (five) presented largely positive results. All of the reports in this category were received from the United Nations Office on Drugs and Crime. Key points emerging from these evaluations included the following: (a) The Organization had increased the understanding of local administrations with respect to drug use prevention; (b) HIV/AIDS prevention programmes had been effective in working with HIV-positive inmates and former detainees; (c) The Organization had successfully promoted the adoption of national legislation and the establishment of law enforcement institutions and procedures through its mentoring and information support systems. Maintenance of international peace and security 52. In the area of the maintenance of international peace and security, five reports received a "good" or "excellent" quality rating and were analysed in terms of their content. Of these, the majority (four) presented largely mixed results. Key points emerging from these evaluations included the following: (a) A successful framework had been established for addressing conduct and discipline issues, in response to requests from peacekeeping missions; (b) There had been an increased number of responses to missions requesting assistance in the monitoring, review and finalization of disciplinary cases, but this had come at the expense of strategic guidance, policy development and the application of procedural standards across missions; (c) Limited progress had been made in terms of workforce planning, and new initiatives were only slowly reducing vacancy rates at United Nations peacekeeping missions; (d) The training of both field and Headquarters staff on United Nations entitlements and rules regarding conditions of service was inadequate; (e) Communication and collaboration between different peacekeeping services and sections was insufficient. Development of Africa 53. In the area of the development of Africa, five reports received a "good" or "excellent" quality rating and were analysed in terms of their content. Of these, the majority (four) presented largely mixed results. Key points emerging from these evaluations included the following: (a) There were significant issues affecting women in Africa; (b) Successes had been achieved in Kenya regarding support for gender gains and the protection of human rights in the new Constitution; (c) A regional network for women in South Africa on law and governance, HIV/AIDS and information and communications technology had had mixed results; (d) Initiatives to end violence in the Central African subregion had strengthened national legal and policy frameworks and contributed to the development and adoption of a national strategy for combating gender-based violence. Promotion of human rights 54. In the area of the promotion of human rights, only one programme, UNWomen, submitted reports, and those two reports received a "good" or "excellent" quality rating. One report presented largely positive results, whereas the other presented mixed results. Given that all programmes are mandated to mainstream human rights into work programmes, the small number of reports received implies a lack of evaluative evidence on how effectively the Organization is promoting human rights. The two reports indicated that: (a) Crucial strides had been achieved regarding awareness-raising and advocacy on human rights and gender-based violence through partnerships with civil society organizations, religious leaders and police in the Sudan and Iraq; (b) A campaign promoting gender equality in the legislative process in Iraq had encouraged women to vote and had been reflected in the larger turnout by women voters in the 2010 constitutional referendum. Disarmament 55. In the area of disarmament, one report was received, and it presented mixed results. The report indicated that not all scheduled disarmament, demobilization and reintegration activities had taken place according to plan during the first disarmament, demobilization and reintegration process, from 2004 to 2008 in Burundi; however, the second process, from 2009 to 2011, had been more favourable. Promotion of justice and international law 56. No evaluation reports were received in the area of the promotion of justice and international law, despite the fact that it is one of the Secretary-General's eight main priority areas. Thus, there is a lack of evaluative evidence on how the Organization is performing in this area. IV. Evaluation workplan of the Office of Internal 57. In order to establish regular budget evaluation priorities, OIOS utilized a systematic, strategic risk-based planning approach introduced in 2007, incorporating 12 proxy risk indicators. It has also used a risk-based planning approach in its peacekeeping evaluations. 58. Since the beginning of 2008, OIOS has completed the following evaluations: Entity evaluations: :: 2008: Department of Political Affairs (one summary and five subprogramme reports), Office of Human Resources Management, Peacebuilding Support Office :: 2009: Office of the United Nations High Commissioner for Human Rights, Office of the Special Adviser on Africa, Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States, Department for General Assembly and Conference Management (integrated global management), United Nations Operation in Côte d'Ivoire :: 2010: Department of Management (six subprogramme reports), UNRWA, United Nations Mission in Liberia :: 2011: Department of Economic and Social Affairs (1 summary and 10 subprogramme reports), United Nations Mission in the Sudan :: 2012: United Nations Stabilization Mission in Haiti Thematic evaluations: :: 2008: review of results-based management :: 2009: coordinating bodies; lessons learned: protocols and practices, Environment Management Group :: 2010: gender mainstreaming in the United Nations Secretariat; United Nations Secretariat business partnerships addressing climate change :: 2011: Department of Peacekeeping Operations/Department of Field Support cooperation with regional organizations; review of the organizational framework of the public information function of the Secretariat 59. By the end of 2013, the following evaluations will have been completed: :: Office for the Coordination of Humanitarian Affairs :: UNEP :: United Nations Office on Drugs and Crime :: ECA :: Review of the evaluation capacity of the Office of the United Nations High Commissioner for Refugees :: Flexibility and adaptability in United Nations Interim Force in Lebanon peacekeeping :: Measures used by peacekeeping missions to report progress on the protection of citizens 60. In the previous biennial report (A/66/71), OIOS proposed that all Secretariat programmes be evaluated according to a 12-year cycle. The General Assembly, in its resolution 65/244, approved the reduction of that cycle to every eight years, and OIOS proposes that a phased approach be taken to achieving this. During the biennium 2014-2015, OIOS will complete evaluations of critical management and strategic areas for the following entities: :: Department of Peacekeeping Operations/Department of Field Support (Headquarters) :: Department of Safety and Security :: ESCAP :: The United Nations Human Settlements Programme :: The Economic Commission for Latin America and the Caribbean :: The United Nations Conference on Trade and Development :: The International Trade Centre :: UN-Women It will also complete thematic evaluations on the following topics: :: Monitoring and evaluation of the Millennium Development Goals: lessons learned :: Sexual exploitation and abuse in peacekeeping missions :: Peacekeeping: protection of civilians :: Peacekeeping: standing police force In addition, it will complete an evaluability assessment on the administration of justice. 61. The Committee for Programme and Coordination may consider which evaluations relating to the aforementioned 2014-2015 workplan it would like to review at its fifty-fourth session, in 2015, and request OIOS to undertake any additional evaluations not included in its current workplan. V. Follow-up on the recommended actions of the Committee for Programme and Coordination 62. In the previous biennial study, OIOS committed to two actions to support improvements in the quality of evaluation in the Secretariat. First, the Office shared detailed evaluation report quality assessments with all programmes in August 2011; it intends to do the same with respect to the reports assessed for this biennial study. Secondly, OIOS shared a report template for high-quality evaluations. 63. In the previous biennial study, the Committee for Programme and Coordination recommended that the General Assembly request the Secretary-General to ensure that OIOS evaluation reports also focus on programme impact and results achieved, by improving the methodology for conducting assessments and, in particular, ensuring regular follow-up on the progress made and more comprehensive conclusions. OIOS has taken several actions in response to this recommendation, including: (a) Greater use of quantitative data; (b) More rigorous follow-up with entities being evaluated to discuss feasible recommendations and action plans for their implementation, including the sharing of additional information that may be useful for programmes as they seek to improve their performance. 64. More recently, the following initiatives have been introduced: (a) The development of programme impact pathways to establish a theory of change for all programmes being evaluated and to provide an evaluative framework for determining the strategic evaluation questions to be addressed; (b) Enhanced scoping at the beginning of all evaluations, undertaken in consultation with the entities being assessed, to ensure that evaluation designs are appropriate for answering the key questions and that the evaluation will add value in terms of decision-making, accountability and learning; (c) Plans to enhance the methodology for upcoming triennial reviews to ensure that they focus not only on the implementation of recommendations but also on reasons for non-implementation and further analysis of remaining issues. 65. The Committee also recommended that the General Assembly request the Secretary-General to ensure that a more systematic approach to evaluating activities is adopted by OIOS, including in terms of the better exploitation of complementarities and synergies among all activities and of the strengthening of coordination among all relevant departments. OIOS responded to this recommendation by: (a) Revisiting its risk assessment methodology for determining the evaluation workplan, particularly with regard to cross-cutting thematic evaluations focusing on programme coordination and synergies; (b) Undertaking a more collaborative approach with Secretariat programmes, in particular the Department of Peacekeeping Operations/Department of Field Support, with which work planning is continuous, when finalizing its workplan to ensure that OIOS evaluations are more responsive to information needs and assessment gaps; (c) Enhancing coordination with the Joint Inspection Unit and the Board of Auditors in finalizing its workplan; (d) Participating as a reference group member with respect to initiatives to establish a system-wide evaluation mechanism. VI. Conclusions 66. The General Assembly, in its resolution 67/226, emphasized the importance of having independent, credible and useful evaluation functions, with sufficient resources, and a culture of evaluation that ensures the active use of evaluation findings and recommendations in policy development and improving the functioning of the organizations of the United Nations. This objective has not yet been reached in the Secretariat, where evaluation has yet to become a fully robust and comprehensive function that is integral to how a programme works. Given the critical issues facing the Organization, it is imperative that evidence-based evaluation on programme performance guide the design and implementation of programmes. 67. When conducted in a credible and reliable manner, evaluation has the potential to feed into strategic decision-making to guide the future of the Organization. In this regard, it is an underutilized resource. 68. In resolution 67/226, the General Assembly noted the development of UNEG norms and standards for evaluation and encouraged their use in the evaluation functions of United Nations funds, programmes and specialized agencies, as well as in system-wide evaluations of operational activities for development. Measured in terms of these norms and standards, evaluation in the Secretariat contains critical gaps that need to be addressed if it is to flourish. These include the need for more financial and human resources, more robust evaluation policies, strengthened operational independence on the part of evaluation units, enhanced senior management support and buy-in, stronger evaluation competencies among staff responsible for evaluation, more regular and systematic procedures for the planning, conduct and follow-up of evaluation, and a more vigorous evaluation culture. 69. There is also a clear need for more support and guidance for self-evaluation. While OIOS has a role to play in providing guidance and methodological advice, it is not appropriate for the Office to directly develop capacity for evaluation within Secretariat programmes or to ensure that programmes are equipped with the competencies necessary to undertake credible evaluation. Those functions are more appropriately placed within the programmes themselves, as it is the responsibility of programme managers to ensure that their staff have the competencies necessary to carry out all relevant functions, including self-evaluation. OIOS can provide guidance and support to programmes in the conduct of self-evaluation, including methodological advice. Examples of this have included providing advice on evaluation design, data collection and analysis, and inviting evaluation colleagues to evaluation seminars of the Inspection and Evaluation Division. Currently, no unit dedicated to supporting self-evaluation exists in the Secretariat. 70. OIOS welcomes any recommendations from Member States on how to strengthen this critical evaluation function. List of departments and offices included in the biennial study 1. Department of Economic and Social Affairs 2. Department of Field Support 3. Department for General Assembly and Conference Management 4. Department of Management 5. Department of Political Affairs 6. Department of Public Information 7. Department of Peacekeeping Operations 8. Department of Safety and Security 9. Economic Commission for Africa 10. Economic Commission for Europe 11. Economic Commission for Latin America and the Caribbean 12. Executive Office of the Secretary-General 13. Economic and Social Commission for Asia and the Pacific 14. Economic and Social Commission for Western Asia 15. International Trade Centre UNCTAD/WTO 16. Office for the Coordination of Humanitarian Affairs 17. Office for Disarmament Affairs 18. Office of the United Nations High Commissioner for Human Rights 19. Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States 20. Office of Internal Oversight Services 21. Office of Legal Affairs 22. Office for Outer Space Affairs 23. Office of the Special Adviser on Africa 24. United Nations Conference on Trade and Development 25. United Nations Environment Programme 26. United Nations Human Settlements Programme 27. Office of the United Nations High Commissioner for Refugees 28. United Nations Office on Drugs and Crime 29. United Nations Relief and Works Agency for Palestine Refugees in the Near East 30. United Nations Office at Geneva 31. United Nations Office at Nairobi 32. United Nations Office at Vienna 33. United Nations Entity for Gender Equality and the Empowerment of Women Annex II In identifying 2010-2011 evaluation reports, the Office of Internal Oversight Services (OIOS) requested all focal points to submit reports finalized in 2010 or 2011 and used programme websites to identify reports. OIOS received a total of 298 reports, representing 28 Secretariat entities. Six entities had no available evaluation reports for the biennium: the Department of Management, the Office of Legal Affairs, the Office for Outer Space Affairs, the Office of the Special Adviser on Africa, the United Nations Office at Vienna and the Office of the High Representative for the Least Developed Countries, Landlocked Developing Countries and Small Island Developing States. Eleven evaluations that had been conducted in languages other than English or had been conducted by donor agencies were excluded from the review. OIOS reviewed the 298 reports to verify that they met the OIOS operational definition of evaluation. A total of 153 were determined to be evaluation reports. None of the reports submitted by the following 10 entities passed the OIOS screening: the Executive Office of the Secretary-General, the Department for General Assembly and Conference Management, the Department of Political Affairs, the Department of Public Information, the Department of Safety and Security, the Economic Commission for Africa, the Economic Commission for Europe, the Office for Disarmament Affairs, the United Nations Office at Geneva and United Nations Office at Nairobi. From among the 153 evaluation reports, a two-tier purposive sampling of 94 reports was conducted for further assessment. All reports from entities submitting 10 or fewer reports were included in the sample. From among the reports of entities submitting more than 10 reports, a further purposive sample was drawn using the following criteria: (a) A balance of reports from each biennium year; (b) A balance in midterm and final evaluations; (c) A mix of project, subprogramme, thematic and external evaluations; (d) A balance of different topics and themes; (e) Widespread geographical coverage. All 94 reports in the sample were assessed to determine the following attributes: (a) Quality; (b) Focus; (c) Scope. The quality assessment used 27 standards. Several report sections were assessed to ascertain quality, including executive summary, introduction, methodology, background, findings, conclusions, recommendations, annexes and format. In order to ensure that the quality assessment was as impartial as possible, OIOS contracted with an independent evaluation expert for its conduct. In addition, the 64 reports assessed to be of "excellent" or "good" quality were categorized under one of the eight strategic priority areas of the Organization and assessed to determine key results and conclusions: (a) Maintenance of international peace and security; (b) Disarmament; (c) Development of Africa; (d) Promotion of human rights; (e) Promotion of sustained economic growth and sustainable development; (f) Effective coordination of humanitarian assistance efforts; (g) Promotion of justice and international law; (h) Drug control, crime prevention and combating international terrorism. Annex III Comments received from Secretariat entities on the draft report Department of Economic and Social Affairs Paragraph 18 of the report states that the lack of a dedicated unit for evaluation in programmes with significant operational activities and mandates, such as the Department of Political Affairs and the Department of Economic and Social Affairs, is of serious concern. The Office of Internal Oversight Services (OIOS) will provide further information and suggestions regarding how to fill this gap in the forthcoming evaluation scorecards. This paragraph was added to the report after the informal draft of the report had been shared by OIOS for comments. The Department looks forward to receiving further information and suggestions from OIOS, especially with regard to the mandate and resources for the establishment of a dedicated evaluation unit. In 2011, the Department set up a departmental network of evaluation focal points that facilitates programme monitoring and evaluation in the Department. The network undertook several training activities. It also contributed to the finalization of the Department's evaluation policy in June 2012. Department of Field Support See Department of Peacekeeping Operations. Department for General Assembly and Conference Management The Department for General Assembly and Conference Management welcomes the report and is greatly appreciative of the effort by OIOS and of the quality and breadth of coverage of the report. Further, the Department would like to point out that, given its mandate, it is particularly interested in developing an evaluation framework, together with related policies and integrated tools, that would respond to the inclusion of persons with disabilities in United Nations meetings. A comprehensive evaluation framework will be developed to engage various duty stations in a policy of inclusion and, through them, host countries as they prepare to host international meetings and conferences of the United Nations towards the delivery of the commitments to human rights instruments such as the Convention on the Rights of Persons with Disabilities. The framework will be designed on the basis of lessons learned by the Department in successfully delivering the United Nations Conference on Sustainable Development, for which the Secretariat developed accessibility guidelines to support the Desktop Publishing Unit in publishing in an advisory capacity, including policies, guidelines, implementation and introductions to working and testing tools. The Integrated Sustainable PaperSmart Services Portal was made accessible in accordance with the Web Content Accessibility Guidelines 2.0 under the Web Accessibility Initiative. Accessibility identifiers were used to identify accessible documents. Working closely with the Government of Brazil, as the host country, the Department facilitated the use of sign language (Libras) and communication access real-time translation. Department of Peacekeeping Operations Paragraph 9 1. The Department of Peacekeeping Operations requests that OIOS specify the multiple sources of data used in triangulation. Paragraph 47 2. We request that the last sentence in paragraph 47, on the quality of the Secretariat evaluations, be deleted, as it seems to be a subjective opinion without supporting data. Paragraph 55 3. We request that the heading above paragraph 55 be reworded to read "Disarmament, demobilization and reintegration" instead of "Disarmament" and that the word "only" be removed from the first line of the paragraph, as it stands in contrast to the otherwise factual character of the paragraph. 4. We request that part of paragraph 55 be reworded to read: "not all scheduled activities had taken place according to plan during the first disarmament, demobilization and reintegration process, from 2004 to 2008, in Burundi; however, the second process, from 2009 to 2011, had been more favourable". 5. We also request that paragraph 55 be cross-referenced to the following report, on which it is based, by means of a footnote: "Expanded After Action Review: Disarmament, Demobilization and Reintegration in Burundi, 2000-2011. A retrospective overview of successive DDR processes in Burundi, an impact assessment of program outcomes, and a strategic analysis of institutional partnerships." Economic and Social Commission for Western Asia 1. The report is clear and well supported in its strategic message: there are successful cases of evaluation exercises to build on in order to overcome the general deficiency from which we suffer, especially in collecting and documenting evidence on the results, outcome and impact of our activities Secretariat-wide. It is a message that we will take to heart. 2. At the Economic and Social Commission for Western Asia (ESCWA), we find that our experience is largely in line with the draft report's overall assessments: (a) Improving the quality and usefulness of evaluation is a priority; (b) There is a need to increase the funds allocated to evaluation, which are insufficient (at ESCWA, the share of the budget allocated to evaluation has in fact risen from approximately 0.7 per cent to 1.1 per cent (see figure I)); (c) There is a need to emphasize the complementarity of and distinctions between the functions of monitoring and evaluation; (d) Self-evaluation is undersupported by the Secretariat; (e) There is a need to establish a clearer link among good management, results-based management and the strengthening of evaluation activities; (f) There is a need for strong follow-up to translate evaluation recommendations into actionable management improvement opportunities. 3. In addition to the overall diagnoses and suggestions of the draft report, we have noted the data specific to ESCWA and have no changes to suggest. With respect to the work programme, ESCWA is not on your upcoming schedule. We are sure that this is linked to the eight-year cycle. We at ESCWA will look forward to an opportunity to work with OIOS on an external evaluation of our programme. Office for the Coordination of Humanitarian Affairs The Office for the Coordination of Humanitarian Affairs welcomes the findings of this report and the identification of areas in which evaluation capacity across the Secretariat may be strengthened. The Office has provided comments on the informal report and has no additional comments or concerns to raise at this time. United Nations Human Settlements Programme The report is very useful and timely, given the current debate among United Nations entities, development partners and Member States on strengthening the role of evaluation. The United Nations Human Settlements Programme (UN-Habitat) supports the findings, conclusions and recommendations of the draft report. UN-Habitat has made significant progress in strengthening its evaluation function. In February 2012, we established an independent Evaluation Unit to further strengthen the evaluation function, and in January 2013 we approved the UN-Habitat Evaluation Policy. The Policy gives us an institutional framework for UN-Habitat evaluations, strengthens systematic learning from the work of UN-Habitat and helps to credibly document the effectiveness of our programmes and projects. The draft reveals that the ambition of having a culture of evaluation that ensures an independent, credible and useful evaluation function has not yet been attained in the United Nations Secretariat. It points out the inadequacy of the existing evaluation capacity and insufficient evaluation resources. In UN-Habitat, as in other United Nations agencies, we are faced with the challenge of managing the expectations of evaluation with limited financial and human resources, and we would welcome your views and suggestions on this fundamental challenge. UN-Habitat is committed to further strengthening its evaluation function, creating an evaluation culture and improving performance and effectiveness in this area. In that regard, we look forward to the evaluation of UN-Habitat by OIOS in the biennium 2014-2015 on critical management and strategic issues. United Nations Entity for Gender Equality and the Empowerment of Women The Evaluation Office of the United Nations Entity for Gender Equality and the Empowerment of Women (UN-Women) would like to express thanks to OIOS for this report, which provides a strong case for strengthening evaluation in the Secretariat. The analysis regarding the trends in evaluation design, process and reports was very informative. UN-Women is happy to note the increase in the number of focal points that reported that they are integrating gender and human rights into evaluation design and process. However, as the report indicates, the Secretariat still has a lot of work to do to ensure that gender and human rights are incorporated into evaluation adequately. In this regard, we would like to note that the Evaluation Office has a role in supporting the coordination and reporting of progress made in fulfilling the commitments of the United Nations system on gender equality and the empowerment of women, including document CEB2006/2 and the associated United Nations system-wide action plan. The Evaluation Office is supporting compliance with and reporting on the plan's evaluation performance indicator. During 2012, UN-Women developed a tool for aligning and harmonizing United Nations system reporting with the evaluation performance indicator, and supported the refinement of the associated technical notes. The Evaluation Office coordinated the review of both products by the United Nations Evaluation Group (UNEG) Human Rights and Gender Equality Task Force and will seek endorsement of the revised technical notes and reporting tool by UNEG in the upcoming UNEG annual general meeting 2013. This tool will enable systematic reporting across the United Nations system on the integration of gender equality into evaluation. In terms of building an organizational culture that is supportive of evaluation, we fully agree that it is critical to ensure senior management buy-in to ensure that evaluation is utilized to its fullest potential, as a tool for enhancing the work of the Organization and achieving results for gender equality and women's empowerment. Sufficient financial and human resources are also necessary for an evaluation function to cover the entire programme of work of an entity. The Evaluation Office also believes that it is critical to build effective planning, monitoring and reporting systems that will enable the evaluation function to provide the most useful information possible and contribute to evidence-based programming. 
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Demo\n",
    "\n",
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.manifold import TSNE\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import save\n",
    "from bokeh.models import HoverTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus processing\n",
    "\n",
    "20 Newsgroups data set. Set of 20,000 newsgroup documents spread accross 20 different topics. This makes it a nice testing ground for topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 500\n",
    "n_top_words = 5\n",
    "threshold = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=remove)\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "corpus_raw = [' '.join(filter(str.isalpha, raw.lower().split())) for raw in\n",
    "        newsgroups.data]\n",
    "# print(newsgroups_train.data)\n",
    "print(\"Before:\\n\", newsgroups.data[0])\n",
    "print(\"After:\\n\", corpus_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation) \n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "def cleaning(article):\n",
    "    one = \" \".join([i for i in article.split() if i not in stopwords])\n",
    "    two = \"\".join(i for i in one if i not in punctuation)\n",
    "    three = \" \".join(lemmatize.lemmatize(i) for i in two.split())\n",
    "    four = three.split(\" \")\n",
    "    return four\n",
    "\n",
    "corpus_tokenized = [cleaning(doc) for doc in corpus_raw]\n",
    "print(corpus_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BOW matrix for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO,\n",
    "                   filename='running.log',filemode='w')\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(corpus_tokenized)\n",
    "\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus_tokenized]\n",
    "\n",
    "print(\"Len of raw corpus: %i | Len of matrix: %i\" % (len(corpus_raw), len(doc_term_matrix)))\n",
    "print(\"Processed:\\n\", doc_term_matrix[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model\n",
    "\n",
    "This can take a couple minutes. I chose to use the actual number of topics for the sake of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "start = time()\n",
    "# Creating the object for LDA model using gensim library\n",
    "# Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Get topics\n",
    "num_topics = len(newsgroups.target_names)\n",
    "print(num_topics)\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = LdaModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary, passes=50)\n",
    "print('used: {:.2f}s'.format(time()-start))\n",
    "\n",
    "ldamodel.save('topic.model')\n",
    "\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Load model if you already have a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads saved model\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "loaded_model = LdaModel.load('topic.model')\n",
    "\n",
    "print(loaded_model.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics\n",
    "for i in ldamodel.print_topics(): \n",
    "    for j in i: print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets test it out on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_test = fetch_20newsgroups(subset='all', remove=remove)\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "corpus_raw_test = [' '.join(filter(str.isalpha, raw.lower().split())) for raw in\n",
    "        newsgroups_test.data]\n",
    "\n",
    "print(\"Original Sentence:\\n\", newsgroups_test.data[0])\n",
    "\n",
    "corpus_tokenized_test = [cleaning(doc) for doc in corpus_raw_test]   \n",
    "\n",
    "doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in corpus_tokenized_test]\n",
    "\n",
    "print(\"\\nAfter processing:\\n\", doc_term_matrix_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example output for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = loaded_model[doc_term_matrix_test[100]]\n",
    "print(\"Output:\\n\", test_output)\n",
    "for i in test_output:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "## Get vectors\n",
    "\n",
    "We have to predict the probabilities for each document and put them in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_matrix = np.zeros((len(doc_term_matrix_test), num_topics))\n",
    "\n",
    "for i, doc in enumerate(doc_term_matrix_test):\n",
    "    predictions = loaded_model[doc]\n",
    "    idx, prob = zip(*predictions)\n",
    "    prob_matrix[i, idx] = prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "20 dimentions are hard to visualize, so lets run t-SNE to reduce the dimentionality. This can also take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_idx = np.amax(prob_matrix, axis=1) > threshold  # idx of news that > threshold\n",
    "_topics = prob_matrix[_idx]\n",
    "\n",
    "num_example = len(_topics)\n",
    "\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99,\n",
    "                  init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(_topics[:num_example])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up metadata for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most probable topic for each news\n",
    "_lda_keys = []\n",
    "for i in range(_topics.shape[0]):\n",
    "    _lda_keys += _topics[i].argmax(),\n",
    "\n",
    "# show topics and their top words\n",
    "topic_summaries = []\n",
    "for i in range(num_topics):\n",
    "    word, _ = zip(*loaded_model.show_topic(i, topn=n_top_words))\n",
    "    topic_summaries.append(' '.join(word))\n",
    "\n",
    "\n",
    "# 20 colors\n",
    "colormap = np.array([\n",
    "  \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "  \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "  \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "  \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])\n",
    "\n",
    "title = \"[20 newsgroups] t-SNE visualization of LDA model trained on {} news, \" \\\n",
    "        \"{} topics, thresholding at {} topic probability, {} iter ({} data \" \\\n",
    "        \"points and top {} words)\".format(\n",
    "  prob_matrix.shape[0], num_topics, threshold, n_iter, num_example, n_top_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, CDSView\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook() \n",
    "\n",
    "p = bp.figure(plot_width=1400, plot_height=1100,\n",
    "                     title=title,\n",
    "                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "  x=tsne_lda[:,0],\n",
    "  y=tsne_lda[:, 1],\n",
    "  color=colormap[_lda_keys][:num_example],\n",
    "  content=corpus_raw_test[:num_example],\n",
    "  topic_key=_lda_keys[:num_example]\n",
    "  )\n",
    ")\n",
    "\n",
    "p.scatter(x='x', y='y', color='color', source=source)\n",
    "\n",
    "topic_coord = np.empty((prob_matrix.shape[1], 2)) * np.nan\n",
    "for topic_num in _lda_keys:\n",
    "  if not np.isnan(topic_coord).any():\n",
    "    break\n",
    "  topic_coord[topic_num] = tsne_lda[_lda_keys.index(topic_num)]\n",
    "\n",
    "# plot crucial words\n",
    "for i in range(prob_matrix.shape[1]):\n",
    "  p.text(topic_coord[i, 0], topic_coord[i, 1], [topic_summaries[i]])\n",
    "\n",
    "# hover tools\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"@content - topic: @topic_key\"}\n",
    "\n",
    "# p.scatter(x=tsne_lda[:,0], y=tsne_lda[:, 1], color=colormap[_lda_keys][:num_example])\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
